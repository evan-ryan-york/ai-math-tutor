# AI Math Tutor - Claude Context File

## Project Overview

This is an **AI-powered math tutoring application** that combines real-time voice conversations with an interactive whiteboard to teach mathematical concepts through stage-based lessons.

**Current Status:** Production-ready with a complete division-with-remainders lesson implemented.

## Quick Start for Claude

### What This Project Does

Students learn math through:
1. **Voice conversations** with an AI tutor (OpenAI Realtime API via WebRTC)
2. **Interactive whiteboard** where they can draw solutions
3. **Stage-based progression** that advances when learning objectives are mastered
4. **Visual annotations** generated by AI to support learning

### Key Technologies

- **Frontend**: React 18 + TypeScript + Vite
- **Voice AI**: OpenAI `gpt-realtime` (Aug 2025 model)
- **Vision AI**: Google Gemini 2.5 Flash (Sept 2025 model)
- **Backend**: Vercel Serverless Functions
- **Connection**: WebRTC for low-latency audio streaming

### Project Structure at a Glance

```
ai-math-tutor/
├── src/
│   ├── App.tsx                    # Main app logic & orchestration
│   ├── components/
│   │   ├── VoiceInterface.tsx    # WebRTC connection setup
│   │   ├── Whiteboard.tsx        # Canvas drawing & AI rendering
│   │   └── StageDisplay.tsx      # Shows current lesson stage
│   ├── data/
│   │   └── lessons.ts            # Lesson definitions & stages
│   └── types/index.ts            # TypeScript types
├── api/
│   ├── session.ts                # WebRTC session creation (OpenAI proxy)
│   └── render.ts                 # Whiteboard rendering (Gemini vision)
└── docs/ai/                      # Detailed architecture docs
```

## Important Files to Know

### `src/App.tsx` (Main Application Logic)
The orchestrator for the entire application. Contains:
- Session management and WebRTC connection handling
- Stage progression logic
- Event handling for AI function calls (`stage_complete`, `update_whiteboard`)
- VAD settings (threshold: 0.6, silence: 1000ms) - **tuned to prevent AI self-interruption**

### `src/data/lessons.ts` (Lesson Definitions)
Contains structured lesson data with:
- Stage-by-stage problem descriptions
- Learning objectives and mastery criteria
- Context instructions for the AI tutor
- Currently has 1 complete lesson: "division-with-remainders-1"

### `api/session.ts` (Backend: Session Creation)
Serverless function that:
- Accepts WebRTC SDP offer from frontend
- Proxies request to OpenAI Realtime API with API key
- Configures session with voice model and stage context
- Returns SDP answer for WebRTC connection

### `api/render.ts` (Backend: Whiteboard Rendering)
Serverless function that:
- Receives whiteboard screenshot + natural language description
- Uses Gemini 2.5 Flash to analyze image and generate drawing commands
- Returns JSON array of canvas drawing primitives
- Enables AI to draw on the whiteboard

## Environment Variables Required

```bash
# .env.local (never commit this!)
OPENAI_API_KEY=sk-proj-xxxxx      # For realtime voice tutoring
GOOGLE_API_KEY=xxxxx              # For whiteboard vision analysis
```

Both keys are **required** for full functionality.

## Development Commands

```bash
pnpm install          # Install dependencies
pnpm run dev          # Start Vercel dev server (includes API functions)
pnpm run build        # TypeScript check + production build
```

**Important:** Always use `pnpm run dev` (which runs `vercel dev`), not plain `vite`, to ensure API endpoints work locally.

## Architecture Flow

```
Browser → WebRTC → OpenAI Realtime API (voice tutoring)
   ↓
Canvas drawing captured
   ↓
POST /api/render → Gemini 2.5 Flash → Drawing commands
   ↓
Execute on canvas → AI sees updated whiteboard
```

## Key Implementation Details

### Voice Activity Detection (VAD)
Located in `src/App.tsx:92-97`:
```typescript
turn_detection: {
  type: 'server_vad',
  threshold: 0.6,           // Prevents AI self-interruption
  prefix_padding_ms: 300,
  silence_duration_ms: 1000
}
```

**Why it matters:** These settings were specifically tuned to prevent the AI from detecting its own voice and interrupting itself mid-sentence.

### Stage Progression
The AI tutor calls `stage_complete(reasoning)` when it determines the student has demonstrated mastery. The app:
1. Loads the next stage from the lesson data
2. Updates the UI with new problem and objectives
3. Sends new context to the AI via `session.update`
4. AI continues tutoring with new stage instructions

### Whiteboard Sync
When students draw:
1. Canvas captures image as data URL (debounced after 1 second)
2. Image sent to AI via data channel (`conversation.item.create` with `input_image`)
3. AI can see the drawing and reference it in conversation
4. AI can also request annotations via `update_whiteboard()` function call

## Common Development Tasks

### Adding a New Lesson
Edit `src/data/lessons.ts` and add a new lesson object with stages. Each stage needs:
- `problem`: The problem text presented to student
- `learning_objective`: What this stage teaches
- `mastery_criteria`: Signals that student is ready to advance
- `context_for_agent`: Instructions for the AI tutor's behavior

### Adjusting AI Tutor Behavior
Modify the `context_for_agent` field in lesson stages or the system instructions in `src/App.tsx:88`.

### Debugging WebRTC Issues
Check the console for:
- `data_channel.readyState` should be "open"
- Look for `Server event:` logs to see OpenAI events
- `input_audio_buffer.speech_started` during AI speech = self-interruption issue

## Current Branch Structure

- **`main`**: Clean initial commit (stable baseline)
- **`orchestrator-setup`**: Active development branch with all features
  - VAD sensitivity fixes
  - Complete lesson implementation
  - Enhanced README and documentation

## Documentation Resources

| Document | Purpose |
|----------|---------|
| `README.md` | Complete setup guide for developers |
| `docs/ai/feature-details.md` | Full implementation guide (1400+ lines) |
| `docs/ai/model-choices.md` | AI model selection rationale |
| `.env.example` | Template for required environment variables |

## Known Issues / Solutions

### "AI keeps interrupting itself"
- **Cause:** VAD threshold too low, AI's voice triggers speech detection
- **Solution:** Increase `threshold` in `turn_detection` config (currently 0.6)

### "API endpoints return 404"
- **Cause:** Using `vite` instead of `vercel dev`
- **Solution:** Use `pnpm run dev` which runs `vercel dev`

### "No audio playback"
- **Cause:** Browser permissions or WebRTC support
- **Solution:** Use Chrome/Edge, ensure microphone permissions granted

## Testing the App Locally

1. Start server: `pnpm run dev`
2. Open browser to `http://localhost:3000`
3. Click "Start Lesson" → "Start Voice Session"
4. Allow microphone access
5. AI will present the pizza division problem
6. Speak naturally to respond
7. Draw on whiteboard to show work

## Cost per Session

- **Voice (10 min):** ~$1.44 (OpenAI Realtime API)
- **Whiteboard:** ~$0.01 (Gemini Flash, ~5 requests)
- **Total:** ~$1.45 per 10-minute tutoring session

## What Makes This Project Unique

1. **Direct WebRTC to OpenAI:** No backend audio proxy needed, extremely low latency
2. **Multimodal AI orchestration:** Different AI models for different tasks (voice vs vision)
3. **Stage-based mastery:** AI decides when to advance, not timers or fixed scripts
4. **Bi-directional whiteboard:** Student draws, AI sees and can annotate
5. **Serverless architecture:** Scales automatically, minimal infrastructure

## Current Lesson: Division with Remainders

**Scenario:** Sharing pizzas fairly

- **Stage 1:** 16 slices ÷ 3 people → discover remainders
- **Stage 2:** 2 types of pizza (cheese + pepperoni) → dividing each separately

The AI tutor adapts its explanations based on student responses and can use the whiteboard to visualize concepts.

## Tips for Working with This Codebase

1. **Always test with audio:** The core experience is voice-based
2. **Check event logs:** `Server event:` logs show what's happening with WebRTC
3. **Understand data channel events:** That's how the frontend communicates with OpenAI beyond audio
4. **Stage context is critical:** The `context_for_agent` field guides AI behavior for each stage
5. **VAD settings matter:** Small changes dramatically affect conversation quality

## Next Steps / Roadmap

Future enhancements could include:
- Multiple lessons with a lesson selection UI
- Student progress tracking and analytics
- Session history and replay
- Mobile/tablet optimization
- Text-based fallback for students without microphones
- Teacher dashboard for monitoring progress

---

**Last Updated:** 2025-10-14
**Primary Branch:** `orchestrator-setup`
**Status:** ✅ Production-ready, actively maintained
